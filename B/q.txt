13/11/05 06:17:49 INFO Slf4jEventHandler: Slf4jEventHandler started
13/11/05 06:17:49 INFO SparkEnv: Registering BlockManagerMaster
13/11/05 06:17:49 INFO MemoryStore: MemoryStore started with capacity 323.9 MB.
13/11/05 06:17:49 DEBUG DiskStore: Creating local directories at root dirs '/tmp'
13/11/05 06:17:49 INFO DiskStore: Created local directory at /tmp/spark-local-20131105061749-d419
13/11/05 06:17:49 INFO ConnectionManager: Bound socket to port 50475 with id = ConnectionManagerId(drone.cs.washington.edu,50475)
13/11/05 06:17:49 INFO BlockManagerMaster: Trying to register BlockManager
13/11/05 06:17:49 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager drone.cs.washington.edu:50475 with 323.9 MB RAM
13/11/05 06:17:49 INFO BlockManagerMaster: Registered BlockManager
13/11/05 06:17:49 INFO HttpBroadcast: Broadcast server started at http://128.208.2.54:36821
13/11/05 06:17:49 INFO SparkEnv: Registering MapOutputTracker
13/11/05 06:17:49 INFO HttpFileServer: HTTP File server directory is /tmp/spark-57122282-981a-4654-86cd-420cacd1b327
13/11/05 06:17:50 INFO SparkUI: Started Spark Web UI at http://drone.cs.washington.edu:4040
13/11/05 06:17:50 DEBUG Configuration: java.io.IOException: config()
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:227)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:214)
	at org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:29)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:241)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:662)

13/11/05 06:17:50 INFO Utils: Copying /homes/network/revtr/ujaved/emulab/spark-0.8.0-incubating/B/helper.py to /tmp/spark-bc5d81a9-42b2-4335-9ef4-fc834985efe7/helper.py
13/11/05 06:17:50 INFO SparkContext: Added file /homes/network/revtr/ujaved/emulab/spark-0.8.0-incubating/B/helper.py at http://128.208.2.54:36239/files/helper.py with timestamp 1383661070260
13/11/05 06:17:50 INFO SparkContext: Input Path : /scratch/ujaved/PCMD/testdir/*.gz
13/11/05 06:17:50 DEBUG Configuration: java.io.IOException: config(config)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:260)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:341)
	at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:349)
	at org.apache.spark.SparkContext.textFile(SparkContext.scala:321)
	at org.apache.spark.api.java.JavaSparkContext.textFile(JavaSparkContext.scala:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:662)

13/11/05 06:17:50 INFO SparkContext: hadoopConfiguration: Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml
13/11/05 06:17:50 INFO SparkContext: hadoopJobConfiguration: Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml
13/11/05 06:17:50 DEBUG Configuration: java.io.IOException: config()
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:227)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:214)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:184)
	at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:236)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:466)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:452)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1494)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1395)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
	at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:542)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:318)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:291)
	at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:352)
	at org.apache.spark.SparkContext.textFile(SparkContext.scala:321)
	at org.apache.spark.api.java.JavaSparkContext.textFile(JavaSparkContext.scala:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:662)

13/11/05 06:17:50 DEBUG Groups:  Creating new Groups object
13/11/05 06:17:50 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000
13/11/05 06:17:50 DEBUG UserGroupInformation: hadoop login
13/11/05 06:17:50 DEBUG UserGroupInformation: hadoop login commit
13/11/05 06:17:50 DEBUG UserGroupInformation: using local user:UnixPrincipal: revtr
13/11/05 06:17:50 DEBUG UserGroupInformation: UGI loginUser:revtr
13/11/05 06:17:50 DEBUG FileSystem: Creating filesystem for file:///
13/11/05 06:17:50 INFO MemoryStore: ensureFreeSpace(36232) called with curMem=0, maxMem=339585269
13/11/05 06:17:50 INFO MemoryStore: Block broadcast_0 stored as values to memory (estimated size 35.4 KB, free 323.8 MB)
13/11/05 06:17:50 DEBUG BlockManager: Put block broadcast_0 locally took  92 ms
13/11/05 06:17:50 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
13/11/05 06:17:50 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
13/11/05 06:17:50 DEBUG NativeCodeLoader: java.library.path=
13/11/05 06:17:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/05 06:17:50 WARN LoadSnappy: Snappy native library not loaded
13/11/05 06:17:50 INFO FileInputFormat: Total input paths to process : 1
13/11/05 06:17:50 DEBUG FileInputFormat: Total # of splits: 1
13/11/05 06:17:50 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2
13/11/05 06:17:50 INFO SparkContext: time: 17754022012658802
13/11/05 06:17:50 INFO SparkContext: partition length: 1
13/11/05 06:17:50 DEBUG DAGScheduler: Got event of type org.apache.spark.scheduler.JobSubmitted
13/11/05 06:17:50 INFO DAGScheduler: class of RDD: class org.apache.spark.api.python.PythonRDD PythonRDD[2] at RDD at PythonRDD.scala:34
13/11/05 06:17:50 INFO DAGScheduler: number of dependencies: 1
13/11/05 06:17:50 INFO DAGScheduler: class of dep: class org.apache.spark.rdd.MappedRDD MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2
13/11/05 06:17:50 INFO DAGScheduler: class of RDD: class org.apache.spark.rdd.MappedRDD MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2
13/11/05 06:17:50 INFO DAGScheduler: number of dependencies: 1
13/11/05 06:17:50 INFO DAGScheduler: class of dep: class org.apache.spark.rdd.HadoopRDD HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2
13/11/05 06:17:50 INFO DAGScheduler: class of RDD: class org.apache.spark.rdd.HadoopRDD HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2
13/11/05 06:17:50 INFO DAGScheduler: number of dependencies: 0
13/11/05 06:17:50 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=false)
13/11/05 06:17:50 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-2)
13/11/05 06:17:50 INFO DAGScheduler: Parents of final stage: List()
13/11/05 06:17:50 DEBUG BlockManager: Got multiple block location in  4 ms
13/11/05 06:17:50 DEBUG BlockManager: Got multiple block location in  0 ms
13/11/05 06:17:50 DEBUG BlockManager: Got multiple block location in  1 ms
13/11/05 06:17:50 INFO DAGScheduler: Missing parents: List()
13/11/05 06:17:50 DEBUG DAGScheduler: submitStage(Stage 0)
13/11/05 06:17:50 DEBUG DAGScheduler: missing: List()
13/11/05 06:17:50 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[2] at RDD at PythonRDD.scala:34), which has no missing parents
13/11/05 06:17:50 DEBUG DAGScheduler: submitMissingTasks(Stage 0)
13/11/05 06:17:50 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[2] at RDD at PythonRDD.scala:34)
13/11/05 06:17:50 DEBUG DAGScheduler: New pending tasks: Set(ResultTask(0, 0))
13/11/05 06:17:50 DEBUG LocalScheduler: parentName:,name:TaskSet_0,runningTasks:0
13/11/05 06:17:50 DEBUG LocalTaskSetManager: availableCpus:8, numFinished:0, numTasks:1
13/11/05 06:17:50 INFO LocalTaskSetManager: Size of task 0 is 3684 bytes
13/11/05 06:17:50 DEBUG DAGScheduler: Got event of type org.apache.spark.scheduler.BeginEvent
13/11/05 06:17:50 DEBUG LocalTaskSetManager: availableCpus:7, numFinished:0, numTasks:1
13/11/05 06:17:50 INFO LocalScheduler: Running 0
13/11/05 06:17:50 INFO LocalScheduler: Fetching http://128.208.2.54:36239/files/helper.py with timestamp 1383661070260
13/11/05 06:17:50 INFO Utils: Fetching http://128.208.2.54:36239/files/helper.py to /tmp/fetchFileTemp4756386868798225630.tmp
13/11/05 06:17:50 DEBUG BlockManager: Getting local block broadcast_0
13/11/05 06:17:50 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, true, 1)
13/11/05 06:17:50 DEBUG BlockManager: Getting block broadcast_0 from memory
13/11/05 06:17:50 DEBUG Configuration: java.io.IOException: config()
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:227)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:214)
	at org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1775)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1327)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1775)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1327)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:135)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1814)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1773)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1327)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:61)
	at org.apache.spark.scheduler.local.LocalScheduler.runTask(LocalScheduler.scala:191)
	at org.apache.spark.scheduler.local.LocalActor$$anonfun$launchTask$1$$anon$1.run(LocalScheduler.scala:68)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)

13/11/05 06:17:51 INFO HadoopRDD: Input split: file:/scratch/ujaved/PCMD/testdir/2013-07-08.18-00.-0400.MMEpcmd.gz:0+82357227
13/11/05 06:17:59 INFO PythonRDD: Times: total = 8347, boot = 168, init = 6, finish = 8173
13/11/05 06:17:59 INFO LocalScheduler: Finished 0
13/11/05 06:17:59 DEBUG DAGScheduler: Got event of type org.apache.spark.scheduler.CompletionEvent
13/11/05 06:17:59 INFO LocalScheduler: Remove TaskSet 0.0 from pool 
13/11/05 06:17:59 INFO DAGScheduler: Completed ResultTask(0, 0)
13/11/05 06:17:59 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished in 8.533 s
13/11/05 06:17:59 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took 8.673740146 s
